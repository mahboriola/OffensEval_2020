{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offenseval_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wKR5_PLPHbO"
      },
      "source": [
        "path = '/data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_classification(dataset, threshold):\n",
        "    classification_values = []\n",
        "\n",
        "    for index, row in dataset.iterrows():\n",
        "        if float(row[2]) > threshold:\n",
        "            classification_values.append('OFF')\n",
        "        else:\n",
        "            classification_values.append('NOT')\n",
        "    dataset.insert(2, column='task_a', value=classification_values)\n",
        "    dataset.drop(axis=1, columns=['average', 'std'], inplace=True)\n",
        "    dataset.set_index('id', inplace=True)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(r'{0}/task_a_distant.tsv'.format(path), sep='\\t', header=0)\n",
        "df = data_classification(df, 0.5)\n",
        "\n",
        "df.to_csv(r'{0}/task_a_0.5.tsv'.format(path), sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(r'{0}/task_a_0.5.tsv'.format(path), sep='\\t', header=0)\n",
        "print(dataset.head())\n",
        "\n",
        "rus = RandomUnderSampler(random_state=0)\n",
        "\n",
        "print(df['task_a'].value_counts())\n",
        "\n",
        "X_resampled, y_resampled = rus.fit_resample(df['text'].values.reshape(-1, 1),\n",
        "                                            df['task_a'].values.reshape(-1, 1))\n",
        "\n",
        "df = pd.DataFrame(data=X_resampled, columns=['text'])\n",
        "df['task_a'] = y_resampled\n",
        "print(dataset.head())\n",
        "print(dataset['task_a'].value_counts())\n",
        "\n",
        "df.to_csv(r'{0}/task_a_0.5_resampled.tsv'.format(path), sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f_XT1bZVHak"
      },
      "source": [
        "Text Classification using Spacy TextCategorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmlghEQZVnLS"
      },
      "source": [
        "import pandas as pd\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7SSRNjlVqwk"
      },
      "source": [
        "def df2list(text_df, label_df):\n",
        "\tls_ = [(text_df.iloc[i], {'cats': label_df.iloc[i].to_dict()}) for i in range(len(text_df))]\n",
        "\treturn ls_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6hY2pBiVzes",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7a1c6c67-5fd9-45d4-d9fd-83d5cfe79a1f"
      },
      "source": [
        "print('reading dataset')\n",
        "train = pd.read_csv(r'{0}/task_a_0.5.tsv'.format(path), sep='\\t', header=0)\n",
        "validation = pd.read_csv(r'{0}/testset_2020.tsv'.format(path), sep='\\t', header=0)\n",
        "\n",
        "print('splitting train test data')\n",
        "X_train = train['text'].copy()\n",
        "y_train = train['task_a'].copy()\n",
        "X_val = validation['text'].copy()\n",
        "y_val = validation['task_a'].copy()\n",
        "\n",
        "y_train = pd.get_dummies(y_train)\n",
        "y_val = pd.get_dummies(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JSGhKtcVE3C"
      },
      "source": [
        "testset = pd.read_csv(r'{0}/test_a_tweets.tsv'.format(path), sep='\\t', header=0)\n",
        "test_labels = pd.read_csv(r'{0}/test_a_baseline.csv'.format(path), header=None)\n",
        "test_labels.columns = ['id', 'task_a']\n",
        "\n",
        "X_test = testset['tweet']\n",
        "y_test = test_labels['task_a']\n",
        "y_test = pd.get_dummies(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3DMaOZhWsFg"
      },
      "source": [
        "train_ls = df2list(X_train, y_train)\n",
        "val_ls = df2list(X_val, y_val)\n",
        "test_ls = df2list(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXsGWoLJWs6M"
      },
      "source": [
        "col_vals = list(train['task_a'].unique())\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "if 'textcat' not in nlp.pipe_names:\n",
        "    textcat = nlp.create_pipe('textcat')\n",
        "    nlp.add_pipe(textcat, last=True)\n",
        "else:\n",
        "    textcat = nlp.get_pipe('textcat')\n",
        "\n",
        "# add label to text classifier\n",
        "for _, col_val in enumerate(col_vals):\n",
        "    textcat.add_label(col_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4teIReU6Wvcu"
      },
      "source": [
        "from spacy.util import minibatch, compounding\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IxMkJn8WxlE"
      },
      "source": [
        "val_text, val_label = list(zip(*val_ls))\n",
        "test_text, test_label = list(zip(*test_ls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HprZFlApW0Yo"
      },
      "source": [
        "output_dir = path + '/spacy_models/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeRDxdJSXFjT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "98f52a2a-754e-4e60-df07-b6960e68e0de"
      },
      "source": [
        "n_iter = 20\n",
        "print_every= 1\n",
        "not_improve = 5 \n",
        "\n",
        "\n",
        "# Train model\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
        "with nlp.disable_pipes(*other_pipes): \n",
        "    optimizer = nlp.begin_training()\n",
        "    \n",
        "    score_f1_best = 0\n",
        "    early_stop = 0\n",
        "    \n",
        "    for i in range(n_iter):\n",
        "        losses = {}\n",
        "        true_labels = list() # true label\n",
        "        pdt_labels = list() # predict label\n",
        "        \n",
        "        random.shuffle(train_ls)\n",
        "        batches = minibatch(train_ls, size=compounding(4., 32., 1.001))\n",
        "        for batch in batches:\n",
        "            texts, annotations = zip(*batch)\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
        "                       losses=losses)\n",
        "            \n",
        "        with textcat.model.use_params(optimizer.averages): \n",
        "            docs = [nlp.tokenizer(text) for text in val_text]\n",
        "            \n",
        "            for j, doc in enumerate(textcat.pipe(docs)):\n",
        "                true_series = pd.Series(valid_label[j]['cats'])\n",
        "                true_label = true_series.idxmax()\n",
        "                true_labels.append(true_label)\n",
        "    \n",
        "                pdt_series = pd.Series(doc.cats)\n",
        "                pdt_label = pdt_series.idxmax()\n",
        "                pdt_labels.append(pdt_label)\n",
        "                \n",
        "            score_f1 = f1_score(true_labels, pdt_labels, average='macro')\n",
        "            score_ac = accuracy_score(true_labels, pdt_labels)\n",
        "            \n",
        "            if i % print_every == 0:\n",
        "                print('textcat loss: {:.4f}\\tf1-score: {:.3f}\\taccuracy: {:.3f}'.format(\n",
        "                    losses['textcat'], score_f1, score_ac))\n",
        "            \n",
        "            if score_f1 > score_f1_best:\n",
        "                early_stop = 0\n",
        "                score_f1_best = score_f1\n",
        "                with nlp.use_params(optimizer.averages):\n",
        "                    nlp.to_disk(output_dir) # save the model\n",
        "            else:\n",
        "                early_stop += 1\n",
        "            \n",
        "            if early_stop >= not_improve:\n",
        "                print('Finished training...')\n",
        "                break\n",
        "            \n",
        "            if i == n_iter:\n",
        "                print('Finished training...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQXQcDSfXJst",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f0b772f-9729-4c69-c9fa-b0cfa541cd58"
      },
      "source": [
        "# Load saved model.\n",
        "nlp = spacy.load(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9KV8EnaXnTY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "import math\n",
        "import seaborn as sns\n",
        "\n",
        "# Evaluate the model.\n",
        "def evaluate(nlp, texts, labels, label_names):\n",
        "\t\"\"\"\n",
        "\t:param nlp: spacy nlp object\n",
        "\t:param texts: list of sentences\n",
        "\t:param labels: dictionary of labels\n",
        "\t:param label_names: list of label names\n",
        "\t\"\"\"\n",
        "\tlabel_names = label_names\n",
        "\ttrue_labels = []\n",
        "\tpdt_labels = []\n",
        "\tdocs = [nlp.tokenizer(text) for text in texts]\n",
        "\ttextcat = nlp.get_pipe('textcat')\n",
        "\tfor j, doc in enumerate(textcat.pipe(docs)):\n",
        "\t\ttrue_series = pd.Series(labels[j]['cats'])\n",
        "\t\ttrue_label = true_series.idxmax()  # idxmax() is the new version of argmax()\n",
        "\t\ttrue_labels.append(true_label)\n",
        "\n",
        "\t\tpdt_series = pd.Series(doc.cats)\n",
        "\t\tpdt_label = pdt_series.idxmax()\n",
        "\t\tpdt_labels.append(pdt_label)\n",
        "\tscore_f1 = f1_score(true_labels, pdt_labels, average='macro')\n",
        "\tscore_ac = accuracy_score(true_labels, pdt_labels)\n",
        "\tprint('f1 score: {:.3f}\\taccuracy: {:.3f}'.format(\n",
        "\t\tscore_f1, score_ac))\n",
        "\n",
        "\tprint('\\nclassification report...')\n",
        "\tprint(classification_report(true_labels, pdt_labels, target_names=label_names))\n",
        "\t\n",
        "\t#data = {'ID': testset['id'],\n",
        "        \t#'LABEL': pdt_labels}\n",
        "\tdata = {'LABEL': pdt_labels}\n",
        "\tresults = pd.DataFrame(data)\n",
        "\t#results.set_index('ID', inplace=True)\n",
        "\tresults.to_csv(r'{0}/labels_testset_last.csv'.format(path), header=None)\n",
        "\tprint('saved labels to csv file')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPSDmlBQXKkf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "607fb415-9273-4632-cae7-7be4cd22aa06"
      },
      "source": [
        "evaluate(nlp, test_text, test_label, label_names=['NOT', 'OFF'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}